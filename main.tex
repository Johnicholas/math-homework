\documentclass[11pt, noamsfonts]{amsart}
\usepackage[left=1.5in, right=1.5in, top=1.2in, bottom=1.2in]{geometry}
\usepackage[charter,expert]{mathdesign}
\usepackage[scaled=.96,osf]{XCharter}% matches the size used in math
\usepackage{hyperref}
\usepackage{syntax} % for a BNF spec

\usepackage{tikz-cd}
\tikzset{>={Straight Barb[length=2pt,width=4pt]}, commutative diagrams/arrow style=tikz}

\linespread{1.04}

\usepackage{enumitem}
\setlist[1]{labelindent=\parindent}
\setlist[enumerate]{labelsep=0.5em}
\setlist[enumerate,1]{label={\upshape (\roman*)}, ref={\upshape (\roman*)}}
\setlist[itemize]{label={--}}

% Point Formats
\newcommand{\pointheader}{\vspace{2mm}\noindent\refstepcounter{section}\textbf{\thesection.}}
\newcommand{\point}{\pointheader~}
\newcommand{\tpoint}[1]{\pointheader~{\bf #1. ---}}
\newcommand{\epoint}[1]{\pointheader~{\em #1.}}
\newcommand{\bpoint}[1]{\pointheader~{\bf #1.}}

% QED Symbol
\newcommand{\psqedsymb}{\(\blacksquare\)}
\renewcommand{\qed}{~\hfill{\psqedsymb}}


\makeatletter
\newcommand*{\coloneqq}{\mathrel{\rlap{%
           \raisebox{0.3ex}{$\m@th\cdot$}}%
           \raisebox{-0.3ex}{$\m@th\cdot$}}%
           =}
\newcommand{\eqqcolon}{=%
           \mathrel{\rlap{%
           \raisebox{0.3ex}{$\m@th\cdot$}}%
           \raisebox{-0.3ex}{$\m@th\cdot$}}}
\makeatother

\DeclareMathOperator{\Th}{Th}
\DeclareMathOperator{\Span}{Span}

\newtheorem{theorem}{Theorem}

\title{Math homework for August 5, 2021}
\begin{document}
\maketitle

\bpoint{Exercise}
Prove that a concrete description of the span of \(S\) is
\[
\mathrm{span}(S) =
\{ a_1v_1 + a_2v_2 + \cdots + a_m v_m :
m \in \mathbf{Z}_{\geq 0},
v_1,\ldots,v_m \in S,
a_1,\ldots,a_m \in k\}
\]
the set of all finite \(k\)-linear combinations of elements of \(S\).\footnote{Hint:
Do so by proving the inclusions \(\subseteq\) and \(\supseteq\) separately.
That \(\supseteq\) holds is due to the minimality condition on
\(\mathrm{span}(S)\), whereas \(\subseteq\) follows by showing all \(U\)
appearing in the intersection must contain the set on this right hand side.}

A linear expression in terms of a set \(S\) is a finite tree consisting of scalar multiplication branches, sum branches, the zero vector, and leaves. Scalar multiplication branches are labeled with a scalar, while leaves are labeled with elements of the set \(S\).

\begin{grammar}

<linear expression> ::= <scalar> <linear expression> 
\alt `(' <linear expression> `+' <linear expression> `)'
\alt `0'
\alt <s>

\end{grammar}

A linear expression can be normalized, using the law of distribution of scalar multiplication, to not contain any sums-below-scalars, and using the law of scalar multiplication, to have at most one scalar (called coefficient) above each leaf. We can introduce scalar multiplication nodes multiplying by the scalar multiplicative unit to create exactly one coefficient above each leaf.  We can introduce sum nodes, zero coefficient nodes, and leaves to have exactly one leaf for each element of the basis. We can eliminate all occurrences of the zero vector. 

TODO: Prove this using an automated termination tool?

Aside: The associative-commutative properties of vector sum means that these normal forms can be viewed as functions from \(S\) to the scalars.

To multiply a linear expression by a scalar, add the scalar multiplication node above the root and normalize. To add a linear expression to another linear expression, add a sum node above the two roots and normalize. So functions from a finite set to the scalars at least form a pre-vector-space; the signature matches. Since the rewrites are sequences of applications of rewrites allowed by the equational theory, these normal forms with these operations do form a vector space.

If there is a linear expression for a point in terms of elements of \(S\), then you can interpret that linear expression as a recipe (or proof) constructing that point using either, for the branches, the guarantees of a vector space, or, for the leaves, the guarantee that \(S \subset \operatorname{span} S\).

The set of points that have a linear expression in terms of elements of \(S\) does form a vector space - linear expressions are exactly those expressions that are closed under the actions of a vector space.

If we have a point that doesn't have a linear expression, then we could intersect it with the space of points that have a linear expression, and show that it is not in the span.

\bpoint{Exercise}
Prove that any two bases of \(V\) have the same size \(\#B\).

I'd like to prove this basic theorem (any two bases have the same size) about vector spaces, and I believe the proof breaks into two halves: First, that finite sets of vectors form matroids, and second, that bases of a matroid all have the same size.

How do you explain the motivation to invent matroids?
Well, there's a thing that happens, when you are doing mathematics.
You have an informal concept, something like,
`I want to find a thing that relates to a familiar thing A in the same way that this familiar thing B relates to this familiar thing C'.
Working with the informal concept, you develop some confidence in some statements which `ought to be true'.

Then you have some questions:
\begin{enumerate}
    \item Do you want to take all of these as definitional?
    \item Can you take only a subset of them as definitional, and derive the others as theorems?
    \item What if there are multiple subsets that are `spanning' - that is, sufficient to derive the entire set?
    \item What if there are symmetries among the different axiomatization? Communicating the symmetries would be important.
    \item How are you going to talk about the situation? The different statements are all grounded in the same informal intuition, but there is some redundancy, and there are some proofs in the various directions.
\end{enumerate}

Fortuntately, there is a mathematical object for this situation - the matroid.

Matroids are named for matrices in linear algebra; the `-oid' suffix means `ish'. That is, they're matrix-ish things. If you imagine the truths (axioms or theorems) in your theory happen to all be linear equations, then there's a correspondence between subsets of truths being redundant, and rows or columns in a matrix, conceived of as a system of linear equations, being linearly dependent.

One of the aspects of the matroid is that there's a set of elements, which you can think of as either true statements or vectors,
and any subset of elements can be categorized in two ways.
The two different categories can be viewed as `similar to the whole' - which in general is redundant, with more theorems than you need to axiomatize the theory in question, or more vectors than you need to span the space in question - or, on the other hand `similar to the empty set' in that there is no redundancy, though these small sets of vectors may span a subspace, or axiomatize a more more generic theory.
If you take a set that was independent, and take something away, you can't introduce a redundancy.
Dually, if you take a set that was dependent (contains a derivation of one of the elements from some other elements) and add something, you can't make it independent.
So this categorization of sets into two kinds is compatible with the inclusion relation of subsets - that is,
it is a monotone map from the full lattice of all subsets of elements to the 2-element lattice; these are sometimes called `abstract simplicial complexes'.

There is another quality to a matroid, beyond just an abstract simplicial complex. Sometimes people describe it as a way to `bump' two sets of elements together to form another set of elements. 

One way to say this `bumping' is to focus on the independent sets that are at the frontier between independence and dependence. These would be sets of vectors that span the entire space or potential axiomatizations of the entire theory, such that adding any more elements would lead to a redundant set. One of these is called a `basis'. Then `basis exchange' is a rule that if there are distinct bases \(A, B\), for any \(a \in A \setminus B\) there exists \(b \in B \setminus A\) such that replacing \(b\) for \(a\) in \(A\) yields another basis.

Another way to describe a `bumping' operation is to focus on the dependent sets that are at the frontier. One of these is called a `circuit'. An alternate description of matroids has a rule that for any circuits \(C_1, C_2\) and \(x\) an element in their intersection, then their union minus \(x\) contains a circuit.

A third way to describe a `bumping' operation is to focus on the independent sets, all of them. In this case, a rule, called `augmentation property' is that if there are two independent sets of different size, then there is a way to pluck an element from the larger, and add it to the smaller, such that the augmented set is an independent set. For the purpose of subsequent proofs, this is the one we will take as definitional of matroids.

There are a few more variations of this general sort, working with `flats', or `hyperplanes', sets of sets of elements, that both pin down the frontier and also have a sort of `bumping' or `exchange' quality. There's another way to describe matroids, which centers a map from sets of elements to natural numbers, called the rank, and yet another way, which centers an endo-function called `closure'. I wouldn't be surprised if there was a dual axiomatization using an endo-function called `interior' (by analogy to topology), or an axiomatization with a binary operation called `implication' or `exponentiation' (by analogy to Heyting algebras).

These alternate descriptions, using different language, are called `cryptomorphisms' - matroids are useful for modeling situations where there are cryptomorphisms, and also matroids have many cryptomorphisms.

The key lemma that finite sets of vectors form matroids is the Steinitz exchange lemma:

\begin{theorem}
If a vector \(z \in \Span (X \cup \{y\})\) then either \(z \in \Span X\) or \(y \in \Span (X \cup \{z\})\).
\end{theorem}

\begin{proof}
Suppose a vector \(z\) is within the space spanned by \(X\) plus a particular vector \(y\).

Look at the coefficient of \(y\) in the linear expression for \(z\).
Either it was zero, in which case \(z\) is within the space spanned by \(X\), or it was nonzero.

By dividing by the nonzero coefficient, we can solve to find a linear expression for \(y\) within the space spanned by \(X \cup \{z\}\).
\end{proof}

Using the Steinitz exchange lemma, we can show:

\begin{theorem}
A finite set of vectors, and linearly independent subsets of that set, constitutes a matroid. 
\end{theorem}

\begin{proof}
First, a subset of a linearly independent set of vectors is also linearly independent. If there are no expressions for one as a combination of the others in a larger set, then there are no expressions for one as a combinations of the others in a particular subset. So the first property, that a matroid needs to be an abstract simplicial complex, is established.

Second, we need to find a way to augment an independent set,
using another, strictly larger, independent set. We will show it by contradiction.

Assume for the sake of contradiction,
\(X\) is a smaller independent set of size \(n\),
and \(Y\) is a larger independent set,
and there is no single element of \(Y\)
that can be added to \(X\) to form a new independent set.

We iterate through  \(Y\), considering
particular elements \(y \in Y\).

In order for these elements of \(Y\) to not augment \(X\), they must be within the space spanned by \(X\). That means they have linear expressions in terms of elements \(x \in X\). For each of the first \(n\), we can solve to find a linear expression for the elements of \(X\) in terms of elements of \(Y\).

Eventually, since \(Y\) is larger, we will be considering the \(n+1\)th element of \(Y\). 

This time, if \(y \in \Span X\), and obtain a linear expression witnessing that, then we can substitute the linear expressions that we have for the previously seen elements of \(Y\),
and derive a linear expression for \(y\) as a linear combination of
other elements of \(Y\), which would contradict the independence of \(Y\).

Therefore the independent sets augmentation property holds, and finite sets of vectors form a matroid.
\end{proof}

I did not invent this proof entirely on my own, I used Wikipedia \url{https://en.wikipedia.org/wiki/Steinitz_exchange_lemma}
and a paper about the logic of functional dependence, \cite{baltag2021simple}.

Aside: I think there might be a variation of this process,
where we keep track of a loop invariant something like `the span
of X can be expressed as the direct sum of the span of these elements (the \(x \in X\) not yet considered) and the span of those elements (the \(y \in Y\) that have been considered)`. With each \(x, y\) pair considered, the dimensions tick over, like this:

Suppose that \(X\) of size \(n\) spans \(V\) and \(Y\) of size \(m\) also spans \(V\). We start with a isomorphism of \(V\) with the direct sum of two spaces-with-bases, \(X_n \oplus 0\). We consider elements of \(Y\) and elements of \(X\) in pairs, and solve to advance the isomorphism one step further, \(X_{n-i} \oplus Y_{i} \cong X_{n-i-1} \oplus Y_{i+1}\). Eventually (after \(n\) steps) we have that \(V \cong 0 \oplus Y_{n} \cong Y_{n}\), and so either \(m = n\) or \(Y\) is not linearly independent.

Finally, we can establish the goal.

\begin{theorem}
All the maximal independent sets in a matroid are the same size.
\end{theorem}

\begin{proof}
If there were maximal independent sets of unequal size, then the smaller could be augmented with an element taken from the larger,
which would be a contradiction of the hypothesis that the smaller was maximal.
\end{proof}


\bpoint{Exercise}
Let \(V\) and \(W\) be vector spaces. Let \(B \subset V\) be a basis of \(V\).
Let \(f \colon B \to W\) be a map of sets. Prove that there exists a unique
linear map \(F \colon V \to W\) such that the restriction of \(F\) to \(B\)
is \(f\).

\begin{proof}
Take a generic element of \(v \in V\).
Since \(B\) is a basis of \(V\),
there exists a linear combination
of \(b \in B\) to create \(v\).
We can substitute \(b \mapsto f b\) in this expression to define a map \(F : V \to W\).

Aside: substitution like this corresponds to composition of functions.

Suppose there was a linear map \(F'\) from \(V\) to \(W\) that restricts to \(f\) on \(B\). Then we can interpret the linear expression in terms of \(f b\) as a recipe to prove that \(F' v = F v\).
The leaves of the linear expression rely on the hypothesis that \(F'\) restricts to \(f\) on \(B\). The coefficients of the linear expression
rely on the functoriality of \(F'\), that it commutes with multiplication by a scalar. The sum of the linear expression also  relies on the functoriality of \(F'\), that it commutes with vector sum.
\end{proof}

\section{How to invent a cryptomorphism}

In order to invent a cryptomorphism, you start with a familiar
mathematical structure, and add a new defined term to it.
Then you prove some theorems about the new defined term, using
the combination of the familiar axioms, and the new definition.
Then you `file off' or `abstract away' the definition, and consider
the mathematical structures that have the new term as part of their given data, subject to some laws (the theorems that you proved).
Then you work backwards and see whether there is a way to define
the old structure in terms of the new. It may not be possible; a counterexample is a result. Iterate.

A vector space has an abelian group (of vectors with addition).
There is also a definable relation of parallelism - that if there exists a scalar that scales one vector to another vector,
then the two vectors are parallel (This idea is not my invention,
see \cite{pierce2009model}, which credits Decartes.)

\[
x \parallel y \Leftrightarrow \exists u \exists v (u x + v y = 0 \wedge (u \neq 0 \vee v \neq 0)
\]

Some theorems:

\begin{theorem}
The zero vector is parallel to everything.
\end{theorem}

\begin{proof}
The zero scalar scales any vector to the zero vector.
\end{proof}

One of the funny things about this definition of parallelism is
that technically witnesses to parallelism are pairs of scalars,
but for nonzero vectors, you only need one scalar to witness parallelism

\begin{theorem}
If \(x \neq 0\) then \(x \parallel y\) if and only if there exists \(u\) such that \(u x = y\).
\end{theorem}

\begin{proof}
If there exists \(u\) such that \(u x = y\), then we can use \(-1\) as \(v\) to show that \(x \parallel y\). 
If there are \(u, v\) showing that \(x \parallel y\), and \(x\) is not the zero vector, then \(v\) is not zero, and we can divide by it to create a single-scalar witness. 
\end{proof}

\begin{theorem}
The set of all vectors parallel to a particular vector is a subspace.
\end{theorem}

\begin{proof}
If the particular vector is zero, then the set of all vectors parallel to it is the whole space. If it is nonzero,
The definition of `parallel to a particular vector' is a special case of `linear expression for', so the set of all vectors parallel to a particular vector is a vector space with a one-element basis.
\end{proof}

\begin{theorem}
Restricted to the set of nonzero vectors, the binary relation of parallelism is an equivalence relation.
\end{theorem}

\begin{proof}
\begin{enumerate}
    \item Parallelism is reflexive: A witness to \(x \parallel x\) is the multiplicative identity scalar.
    \item Parallelism is symmetric. One proof is that the definition is symmetric. Another proof is that a witness to \(x \parallel y\) is a scalar - the multiplicative inverse is a witness to \(y \parallel x\).
    \item Parallelism is transitive, on non-zero vectors. 
    From \(x_1 \parallel x_2\) we get a witness \(k_1 x_1 = x_2\).
    From \(x_2 \parallel x_3\) we get a witness \(k_2 x_2 = x_3\).
    So \(k_1 k_2\) forms a witness to \(x \parallel z\).
\end{enumerate}
\end{proof}

If you took these few theorems as axiomatic of the concept of parallelism, and attempted to add them to the definition of vector spaces, are there any old axioms that become redundant?

This has been an attempt to find a cryptomorphism from vector spaces to abelian groups with parallelism.

\subsection{notes}

\begin{itemize}
\item How to recover the field of scalars: A scalar is an equivalence class of pairs of parallel vectors (the construction is similar to negative numbers as equivalence classes of pairs of naturals). 
\item How to scalar-add scalars (taken from \cite{pierce2009model}):
\(
(x_0, x_1) + (y_0, y_1) = (z_0, z_1)
\)
if and only if there exists \(w\) such that
\( (x_0, w) = (y_0, y_1) \) and \( (x_0, x_1 + w) = (z_0, z_1) \).
\item
How to scalar-multiply scalars (taken from \cite{pierce2009model}):
\( (x_0, x_1) \circ (y_0, y_1) = (z_0, z_1) \)
if and only if there exists \(w\) such that
\(
(y_1, w) = (x_0, x_1 )
\)
and
\(
(y_0, w) = (z_0, z1)
\).
\item
How to scale a vector by a scalar (taken from \cite{pierce2009model}):
\(
(x_0, x_1) y = z
\)
if and only if
\(
(x_0, x_1) = (y, z) 
\)
or both \(y\) and \(z\) are zero.
\end{itemize}

\bibliographystyle{amsplain}
\bibliography{refs}

\end{document}
